diff --git a/lzero/agent/muzero.py b/lzero/agent/muzero.py
index 55dda5d..3e46ae2 100644
--- a/lzero/agent/muzero.py
+++ b/lzero/agent/muzero.py
@@ -71,6 +71,7 @@ class MuZeroAgent:
 
         if cfg is not None and not isinstance(cfg, EasyDict):
             cfg = EasyDict(cfg)
+            print(cfg)
 
         if env_id is not None:
             assert env_id in MuZeroAgent.supported_env_list, "Please use supported envs: {}".format(
diff --git a/lzero/entry/eval_muzero.py b/lzero/entry/eval_muzero.py
index e3e96dc..8a4e738 100644
--- a/lzero/entry/eval_muzero.py
+++ b/lzero/entry/eval_muzero.py
@@ -54,6 +54,7 @@ def eval_muzero(
     evaluator_env.seed(cfg.seed, dynamic_seed=False)
     set_pkg_seed(cfg.seed, use_cuda=cfg.policy.cuda)
 
+    print(type(model))
     policy = create_policy(cfg.policy, model=model, enable_field=['learn', 'collect', 'eval'])
 
     # load pretrained model
diff --git a/lzero/entry/train_muzero.py b/lzero/entry/train_muzero.py
index d090bfe..fbc80d9 100644
--- a/lzero/entry/train_muzero.py
+++ b/lzero/entry/train_muzero.py
@@ -81,9 +81,11 @@ def train_muzero(
         cfg.policy.learn.learner.hook.save_ckpt_after_iter = cfg.policy.eval_freq
 
     policy = create_policy(cfg.policy, model=model, enable_field=['learn', 'collect', 'eval'])
-
+    
+    
     # load pretrained model
     if model_path is not None:
+        print("here")
         policy.learn_mode.load_state_dict(torch.load(model_path, map_location=cfg.policy.device))
 
     # Create worker components: learner, collector, evaluator, replay buffer, commander.
@@ -195,14 +197,7 @@ def train_muzero(
                 break
 
             # The core train steps for MCTS+RL algorithms.
-            try:
-                print("train_muzero.py -- 198")
-                log_vars = learner.train(train_data, collector.envstep)
-                print("log_vars: ", log_vars)
-                print("train_muzero.py -- 200")
-            except Exception as e:
-                print(f"An error occurred: {e}")
-                raise
+            log_vars = learner.train(train_data, collector.envstep)
 
             if cfg.policy.use_priority:
                 replay_buffer.update_priority(train_data, log_vars[0]['value_priority_orig'])
diff --git a/lzero/model/common.py b/lzero/model/common.py
index ed55609..d281d0d 100644
--- a/lzero/model/common.py
+++ b/lzero/model/common.py
@@ -509,6 +509,9 @@ class SimpleDecoder(nn.Module):
         self.up4 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)
         self.norm_out = ImportedNormalize(32)
         self.conv_out = nn.Conv2d(32, out_channels, kernel_size=3, stride=1, padding=1)
+        
+        for params in self.parameters():
+            params.requires_grad_(True)
 
     def forward(self, x):
         x = self.up1(x)
diff --git a/lzero/policy/muzero.py b/lzero/policy/muzero.py
index 0acc66b..09cfdce 100644
--- a/lzero/policy/muzero.py
+++ b/lzero/policy/muzero.py
@@ -64,7 +64,7 @@ class MuZeroPolicy(Policy):
         # (bool) whether to use rnd model.
         use_rnd_model=False,
         # (bool) Whether to use multi-gpu training.
-        multi_gpu=False,
+        multi_gpu=True,
         # (bool) Whether to enable the sampled-based algorithm (e.g. Sampled EfficientZero)
         # this variable is used in ``collector``.
         sampled_algo=False,
@@ -219,7 +219,7 @@ class MuZeroPolicy(Policy):
             by import_names path. For MuZero, ``lzero.model.muzero_model.MuZeroModel``
         """
         if self._cfg.model.model_type == "conv":
-            return 'MuZeroModel', ['lzero.model.muzero_model']
+            return 'MuZeroModel', ['lzero.model.muzero_model_daev']
         elif self._cfg.model.model_type == "mlp":
             return 'MuZeroModelMLP', ['lzero.model.muzero_model_mlp']
         else:
@@ -350,7 +350,7 @@ class MuZeroPolicy(Policy):
         network_output = self._learn_model.initial_inference(obs_batch)
 
         # value_prefix shape: (batch_size, 10), the ``value_prefix`` at the first step is zero padding.
-        latent_state, reward, value, policy_logits = mz_network_output_unpack(network_output)
+        latent_state, reward, value, policy_logits, reconstruction = mz_network_output_unpack(network_output)
 
         # transform the scaled value or its categorical representation to its original value,
         # i.e. h^(-1)(.) function in paper https://arxiv.org/pdf/1805.11593.pdf.
@@ -371,6 +371,8 @@ class MuZeroPolicy(Policy):
         # ==============================================================
         policy_loss = cross_entropy_loss(policy_logits, target_policy[:, 0])
         value_loss = cross_entropy_loss(value, target_value_categorical[:, 0])
+        
+        recon_loss = torch.abs(obs_batch - reconstruction).mean(dim=(1,2,3)) # daev
 
         prob = torch.softmax(policy_logits, dim=-1)
         entropy = -(prob * prob.log()).sum(-1)
@@ -387,7 +389,7 @@ class MuZeroPolicy(Policy):
             # given current ``latent_state`` and ``action``.
             # And then predict policy_logits and value with the prediction function.
             network_output = self._learn_model.recurrent_inference(latent_state, action_batch[:, step_k])
-            latent_state, reward, value, policy_logits = mz_network_output_unpack(network_output)
+            latent_state, reward, value, policy_logits, reconstruction = mz_network_output_unpack(network_output)
 
             # transform the scaled value or its categorical representation to its original value,
             # i.e. h^(-1)(.) function in paper https://arxiv.org/pdf/1805.11593.pdf.
@@ -404,12 +406,14 @@ class MuZeroPolicy(Policy):
 
                     latent_state = to_tensor(latent_state)
                     representation_state = to_tensor(network_output.latent_state)
+                    reconstruction = to_tensor(reconstruction)
 
                     # NOTE: no grad for the representation_state branch
                     dynamic_proj = self._learn_model.project(latent_state, with_grad=True)
                     observation_proj = self._learn_model.project(representation_state, with_grad=False)
                     temp_loss = negative_cosine_similarity(dynamic_proj, observation_proj) * mask_batch[:, step_k]
                     consistency_loss += temp_loss
+                    recon_loss += torch.abs(obs_target_batch[:, beg_index:end_index] - reconstruction).mean(dim=(1,2,3)) # daev
 
             # NOTE: the target policy, target_value_categorical, target_reward_categorical is calculated in
             # game buffer now.
@@ -418,6 +422,8 @@ class MuZeroPolicy(Policy):
             # NOTE: the +=.
             # ==============================================================
             policy_loss += cross_entropy_loss(policy_logits, target_policy[:, step_k + 1])
+            
+            # recon_loss += torch.abs(obs_batch - reconstruction).mean() # daev -- implement comparison to observation_proj?
 
             prob = torch.softmax(policy_logits, dim=-1)
             entropy = -(prob * prob.log()).sum(-1)
@@ -440,10 +446,13 @@ class MuZeroPolicy(Policy):
         # the core learn model update step.
         # ==============================================================
         # weighted loss with masks (some invalid states which are out of trajectory.)
+        
+        # daev -- added reconstruction losses to total loss
+        
         loss = (
                 self._cfg.ssl_loss_weight * consistency_loss + self._cfg.policy_loss_weight * policy_loss +
                 self._cfg.value_loss_weight * value_loss + self._cfg.reward_loss_weight * reward_loss +
-                self._cfg.policy_entropy_loss_weight * policy_entropy_loss
+                self._cfg.policy_entropy_loss_weight * policy_entropy_loss + recon_loss
         )
         weighted_total_loss = (weights * loss).mean()
 
@@ -470,6 +479,7 @@ class MuZeroPolicy(Policy):
             predicted_rewards = torch.stack(predicted_rewards).transpose(1, 0).squeeze(-1)
             predicted_rewards = predicted_rewards.reshape(-1).unsqueeze(-1)
 
+        #daev -- added reconstruction_loss
         return {
             'collect_mcts_temperature': self._collect_mcts_temperature,
             'collect_epsilon': self.collect_epsilon,
@@ -481,6 +491,7 @@ class MuZeroPolicy(Policy):
             'reward_loss': reward_loss.mean().item(),
             'value_loss': value_loss.mean().item(),
             'consistency_loss': consistency_loss.mean().item() / self._cfg.num_unroll_steps,
+            'reconstruction_loss': recon_loss.mean().item(),
             'target_reward': target_reward.mean().item(),
             'target_value': target_value.mean().item(),
             'transformed_target_reward': transformed_target_reward.mean().item(),
@@ -549,7 +560,7 @@ class MuZeroPolicy(Policy):
         with torch.no_grad():
             # data shape [B, S x C, W, H], e.g. {Tensor:(B, 12, 96, 96)}
             network_output = self._collect_model.initial_inference(data)
-            latent_state_roots, reward_roots, pred_values, policy_logits = mz_network_output_unpack(network_output)
+            latent_state_roots, reward_roots, pred_values, policy_logits, _ = mz_network_output_unpack(network_output) # daev -- implement recon?
 
             pred_values = self.inverse_scalar_transform_handle(pred_values).detach().cpu().numpy()
             latent_state_roots = latent_state_roots.detach().cpu().numpy()
@@ -674,7 +685,7 @@ class MuZeroPolicy(Policy):
         with torch.no_grad():
             # data shape [B, S x C, W, H], e.g. {Tensor:(B, 12, 96, 96)}
             network_output = self._collect_model.initial_inference(data)
-            latent_state_roots, reward_roots, pred_values, policy_logits = mz_network_output_unpack(network_output)
+            latent_state_roots, reward_roots, pred_values, policy_logits, _ = mz_network_output_unpack(network_output)
 
             if not self._eval_model.training:
                 # if not in training, obtain the scalars of the value/reward
@@ -742,6 +753,7 @@ class MuZeroPolicy(Policy):
             'reward_loss',
             'value_loss',
             'consistency_loss',
+            'reconstruction_loss',
             'value_priority',
             'target_reward',
             'target_value',
@@ -773,6 +785,7 @@ class MuZeroPolicy(Policy):
             - state_dict (:obj:`Dict[str, Any]`): The dict of policy learn state saved before.
         """
         self._learn_model.load_state_dict(state_dict['model'])
+        print(self._learn_model.load_state_dict(state_dict['model']))
         self._target_model.load_state_dict(state_dict['target_model'])
         self._optimizer.load_state_dict(state_dict['optimizer'])
 
diff --git a/lzero/policy/muzero_daev.py b/lzero/policy/muzero_daev.py
index 6059fcb..09cfdce 100644
--- a/lzero/policy/muzero_daev.py
+++ b/lzero/policy/muzero_daev.py
@@ -372,7 +372,7 @@ class MuZeroPolicy(Policy):
         policy_loss = cross_entropy_loss(policy_logits, target_policy[:, 0])
         value_loss = cross_entropy_loss(value, target_value_categorical[:, 0])
         
-        recon_loss = torch.abs(obs_batch - reconstruction).mean() # daev
+        recon_loss = torch.abs(obs_batch - reconstruction).mean(dim=(1,2,3)) # daev
 
         prob = torch.softmax(policy_logits, dim=-1)
         entropy = -(prob * prob.log()).sum(-1)
@@ -410,9 +410,10 @@ class MuZeroPolicy(Policy):
 
                     # NOTE: no grad for the representation_state branch
                     dynamic_proj = self._learn_model.project(latent_state, with_grad=True)
-                    observation_proj = self._learn_model.project(representation_state, with_grad=False) # daev -- should this be changed to True because now we measure loss?
+                    observation_proj = self._learn_model.project(representation_state, with_grad=False)
                     temp_loss = negative_cosine_similarity(dynamic_proj, observation_proj) * mask_batch[:, step_k]
                     consistency_loss += temp_loss
+                    recon_loss += torch.abs(obs_target_batch[:, beg_index:end_index] - reconstruction).mean(dim=(1,2,3)) # daev
 
             # NOTE: the target policy, target_value_categorical, target_reward_categorical is calculated in
             # game buffer now.
@@ -490,7 +491,7 @@ class MuZeroPolicy(Policy):
             'reward_loss': reward_loss.mean().item(),
             'value_loss': value_loss.mean().item(),
             'consistency_loss': consistency_loss.mean().item() / self._cfg.num_unroll_steps,
-            'reconstruction_loss': recon_loss.mean().item,
+            'reconstruction_loss': recon_loss.mean().item(),
             'target_reward': target_reward.mean().item(),
             'target_value': target_value.mean().item(),
             'transformed_target_reward': transformed_target_reward.mean().item(),
diff --git a/zoo/atari/config/atari_muzero_config.py b/zoo/atari/config/atari_muzero_config.py
index 1e5d49c..7152e47 100644
--- a/zoo/atari/config/atari_muzero_config.py
+++ b/zoo/atari/config/atari_muzero_config.py
@@ -1,7 +1,7 @@
 from easydict import EasyDict
 
 # options={'PongNoFrameskip-v4', 'QbertNoFrameskip-v4', 'MsPacmanNoFrameskip-v4', 'SpaceInvadersNoFrameskip-v4', 'BreakoutNoFrameskip-v4', ...}
-env_id = 'PongNoFrameskip-v4'
+env_id = 'QbertNoFrameskip-v4'
 
 if env_id == 'PongNoFrameskip-v4':
     action_space_size = 6
@@ -23,7 +23,7 @@ evaluator_env_num = 3
 num_simulations = 50
 update_per_collect = 1000
 batch_size = 256
-max_env_step = int(1e6)
+max_env_step = int(2e5)
 reanalyze_ratio = 0.
 eps_greedy_exploration_in_collect = False
 # ==============================================================
@@ -31,7 +31,7 @@ eps_greedy_exploration_in_collect = False
 # ==============================================================
 
 atari_muzero_config = dict(
-    exp_name=f'data_mz_ctree/{env_id[:-14]}_muzero_ns{num_simulations}_upc{update_per_collect}_rr{reanalyze_ratio}_seed0',
+    exp_name=f'daev_data_mz_ctree/{env_id[:-14]}_muzero_ns{num_simulations}_upc{update_per_collect}_rr{reanalyze_ratio}_seed0',
     env=dict(
         stop_value=int(1e6),
         env_id=env_id,
@@ -93,7 +93,7 @@ atari_muzero_create_config = dict(
     policy=dict(
         type='muzero',
         import_names=['lzero.policy.muzero'],
-    ),
+    )
 )
 atari_muzero_create_config = EasyDict(atari_muzero_create_config)
 create_config = atari_muzero_create_config
diff --git a/zoo/atari/config/atari_muzero_multigpu_ddp_config.py b/zoo/atari/config/atari_muzero_multigpu_ddp_config.py
index 536eded..6ddd055 100644
--- a/zoo/atari/config/atari_muzero_multigpu_ddp_config.py
+++ b/zoo/atari/config/atari_muzero_multigpu_ddp_config.py
@@ -17,14 +17,14 @@ elif env_id == 'BreakoutNoFrameskip-v4':
 # ==============================================================
 # begin of the most frequently changed config specified by the user
 # ==============================================================
-gpu_num = 2
+gpu_num = 1
 collector_env_num = 8
-n_episode = int(8*gpu_num)
+n_episode = 16
 evaluator_env_num = 3
 num_simulations = 50
 update_per_collect = 1000
-batch_size = 256
-max_env_step = int(1e6)
+batch_size = 128
+max_env_step = int(2e5)
 reanalyze_ratio = 0.
 eps_greedy_exploration_in_collect = False
 # ==============================================================
diff --git a/zoo/atari/config/daev_data_mz_ctree/Qbert_muzero_ns50_upc1000_rr0.0_ddp_2gpu_seed0_240428_003436/git_diff.txt b/zoo/atari/config/daev_data_mz_ctree/Qbert_muzero_ns50_upc1000_rr0.0_ddp_2gpu_seed0_240428_003436/git_diff.txt
deleted file mode 100644
index d6d1d5e..0000000
--- a/zoo/atari/config/daev_data_mz_ctree/Qbert_muzero_ns50_upc1000_rr0.0_ddp_2gpu_seed0_240428_003436/git_diff.txt
+++ /dev/null
@@ -1,140 +0,0 @@
-diff --git a/lzero/model/common.py b/lzero/model/common.py
-index a5ae328..ed55609 100644
---- a/lzero/model/common.py
-+++ b/lzero/model/common.py
-@@ -35,7 +35,6 @@ class MZNetworkOutput:
-     latent_state: torch.Tensor
-     reconstruction: torch.Tensor
- 
--
- class DownSample(nn.Module):
-             
-     def __init__(self, observation_shape: SequenceType, out_channels: int, activation: nn.Module = nn.ReLU(inplace=True),
-diff --git a/lzero/model/muzero_model_daev.py b/lzero/model/muzero_model_daev.py
-index 1a3e885..65f2b3a 100644
---- a/lzero/model/muzero_model_daev.py
-+++ b/lzero/model/muzero_model_daev.py
-@@ -240,7 +240,7 @@ class MuZeroModel(nn.Module):
-             [0. for _ in range(batch_size)],
-             policy_logits,
-             latent_state,
--            reconstruction,
-+            reconstruction
-         )
- 
-     def recurrent_inference(self, latent_state: torch.Tensor, action: torch.Tensor) -> MZNetworkOutput:
-diff --git a/lzero/model/utils.py b/lzero/model/utils.py
-index c49d3b9..4bd72b1 100644
---- a/lzero/model/utils.py
-+++ b/lzero/model/utils.py
-@@ -51,10 +51,11 @@ def get_reward_mean(model: nn.Module) -> Tuple[np.ndarray, float]:
- 
- def get_params_mean(model: nn.Module) -> Tuple[np.ndarray, float, float, float]:
-     representation_mean = model.representation_network.get_param_mean()
-+    reconstruction_mean = model.decoder.get_param_mean() # daev -- implement?
-     dynamic_mean = model.dynamics_network.get_dynamic_mean()
-     reward_w_dist, reward_mean = model.dynamics_network.get_reward_mean()
- 
--    return reward_w_dist, representation_mean, dynamic_mean, reward_mean
-+    return reward_w_dist, representation_mean, dynamic_mean, reward_mean, reconstruction_mean
- 
- 
- def get_gradients(model: nn.Module) -> List[torch.Tensor]:
-diff --git a/lzero/policy/muzero_daev.py b/lzero/policy/muzero_daev.py
-index bb26d11..6059fcb 100644
---- a/lzero/policy/muzero_daev.py
-+++ b/lzero/policy/muzero_daev.py
-@@ -16,8 +16,6 @@ from lzero.model import ImageTransforms
- from lzero.policy import scalar_transform, InverseScalarTransform, cross_entropy_loss, phi_transform, \
-     DiscreteSupport, to_torch_float_tensor, mz_network_output_unpack, select_action, negative_cosine_similarity, \
-     prepare_obs
--    
--from .utils import reconstruction_calc
- 
- 
- @POLICY_REGISTRY.register('muzero')
-@@ -374,7 +372,7 @@ class MuZeroPolicy(Policy):
-         policy_loss = cross_entropy_loss(policy_logits, target_policy[:, 0])
-         value_loss = cross_entropy_loss(value, target_value_categorical[:, 0])
-         
--        recon_loss = reconstruction_calc(latent_state, obs_batch) # daev
-+        recon_loss = torch.abs(obs_batch - reconstruction).mean() # daev
- 
-         prob = torch.softmax(policy_logits, dim=-1)
-         entropy = -(prob * prob.log()).sum(-1)
-@@ -424,7 +422,7 @@ class MuZeroPolicy(Policy):
-             # ==============================================================
-             policy_loss += cross_entropy_loss(policy_logits, target_policy[:, step_k + 1])
-             
--            recon_loss += reconstruction_calc(latent_state, obs_batch)
-+            # recon_loss += torch.abs(obs_batch - reconstruction).mean() # daev -- implement comparison to observation_proj?
- 
-             prob = torch.softmax(policy_logits, dim=-1)
-             entropy = -(prob * prob.log()).sum(-1)
-@@ -561,12 +559,11 @@ class MuZeroPolicy(Policy):
-         with torch.no_grad():
-             # data shape [B, S x C, W, H], e.g. {Tensor:(B, 12, 96, 96)}
-             network_output = self._collect_model.initial_inference(data)
--            latent_state_roots, reward_roots, pred_values, policy_logits, reconstruction_roots = mz_network_output_unpack(network_output)
-+            latent_state_roots, reward_roots, pred_values, policy_logits, _ = mz_network_output_unpack(network_output) # daev -- implement recon?
- 
-             pred_values = self.inverse_scalar_transform_handle(pred_values).detach().cpu().numpy()
-             latent_state_roots = latent_state_roots.detach().cpu().numpy()
-             policy_logits = policy_logits.detach().cpu().numpy().tolist()
--            reconstruction_roots = reconstruction_roots.detach().cpu().numpy()
- 
-             legal_actions = [[i for i, x in enumerate(action_mask[j]) if x == 1] for j in range(active_collect_env_num)]
-             # the only difference between collect and eval is the dirichlet noise
-@@ -581,8 +578,8 @@ class MuZeroPolicy(Policy):
-                 # python mcts_tree
-                 roots = MCTSPtree.roots(active_collect_env_num, legal_actions)
- 
--            roots.prepare(self._cfg.root_noise_weight, noises, reward_roots, policy_logits, reconstruction_roots, to_play)
--            self._mcts_collect.search(roots, self._collect_model, latent_state_roots, reconstruction_roots, to_play)
-+            roots.prepare(self._cfg.root_noise_weight, noises, reward_roots, policy_logits, to_play)
-+            self._mcts_collect.search(roots, self._collect_model, latent_state_roots, to_play)
- 
-             # list of list, shape: ``{list: batch_size} -> {list: action_space_size}``
-             roots_visit_count_distributions = roots.get_distributions()
-@@ -687,14 +684,13 @@ class MuZeroPolicy(Policy):
-         with torch.no_grad():
-             # data shape [B, S x C, W, H], e.g. {Tensor:(B, 12, 96, 96)}
-             network_output = self._collect_model.initial_inference(data)
--            latent_state_roots, reward_roots, pred_values, policy_logits, reconstruction_roots = mz_network_output_unpack(network_output)
-+            latent_state_roots, reward_roots, pred_values, policy_logits, _ = mz_network_output_unpack(network_output)
- 
-             if not self._eval_model.training:
-                 # if not in training, obtain the scalars of the value/reward
-                 pred_values = self.inverse_scalar_transform_handle(pred_values).detach().cpu().numpy()  # shape（B, 1）
-                 latent_state_roots = latent_state_roots.detach().cpu().numpy()
-                 policy_logits = policy_logits.detach().cpu().numpy().tolist()  # list shape（B, A）
--                reconstruction_roots = reconstruction_roots.detach().cpu().numpy()
- 
-             legal_actions = [[i for i, x in enumerate(action_mask[j]) if x == 1] for j in range(active_eval_env_num)]
-             if self._cfg.mcts_ctree:
-diff --git a/lzero/policy/utils.py b/lzero/policy/utils.py
-index 23c985a..636d8b1 100644
---- a/lzero/policy/utils.py
-+++ b/lzero/policy/utils.py
-@@ -597,18 +597,5 @@ def mz_network_output_unpack(network_output: Dict) -> Tuple:
-     reward = network_output.reward  # shape: (batch_size, support_support_size)
-     value = network_output.value  # shape: (batch_size, support_support_size)
-     policy_logits = network_output.policy_logits  # shape: (batch_size, action_space_size)
--    return latent_state, reward, value, policy_logits
--
--# daev
--
--def reconstruction_calc(recon: torch.Tensor, obs: torch.Tensor) -> torch.Tensor:
--    """
--    Overview:
--        Prepares representation network outputs to be input into decoder
--    Arguments:
--        - network_output (:obj:`Tuple`): the network output of muzero        
--    """
--    
--    loss = torch.abs(obs - recon).mean()
--    
--    return loss
-+    reconstruction = network_output.reconstruction
-+    return latent_state, reward, value, policy_logits, reconstruction
-\ No newline at end of file
\ No newline at end of file
diff --git a/zoo/atari/config/daev_data_mz_ctree/Qbert_muzero_ns50_upc1000_rr0.0_ddp_2gpu_seed0_240428_003436/git_log.txt b/zoo/atari/config/daev_data_mz_ctree/Qbert_muzero_ns50_upc1000_rr0.0_ddp_2gpu_seed0_240428_003436/git_log.txt
deleted file mode 100644
index b43d529..0000000
--- a/zoo/atari/config/daev_data_mz_ctree/Qbert_muzero_ns50_upc1000_rr0.0_ddp_2gpu_seed0_240428_003436/git_log.txt
+++ /dev/null
@@ -1,180 +0,0 @@
-v0.0.5-7-g3669ad0
-
-commit 3669ad0d705c367ad56f91506166c01af95e55d8
-Author: Adam Johnson <ajtaken123@gmail.com>
-Date:   Sat Apr 27 23:57:19 2024 -0400
-
-    debugging
-
- dominique-venv/share/doc/networkx-3.1/LICENSE.txt  |    37 +
- .../networkx-3.1/examples/3d_drawing/README.txt    |     2 +
- .../examples/3d_drawing/mayavi2_spring.py          |    43 +
- .../networkx-3.1/examples/3d_drawing/plot_basic.py |    51 +
- .../share/doc/networkx-3.1/examples/README.txt     |     8 +
- .../networkx-3.1/examples/algorithms/README.txt    |     2 +
- .../examples/algorithms/WormNet.v3.benchmark.txt   | 78736 +++++++++++++++++++
- .../examples/algorithms/hartford_drug.edgelist     |   338 +
- .../examples/algorithms/plot_beam_search.py        |   112 +
- .../algorithms/plot_betweenness_centrality.py      |    83 +
- .../examples/algorithms/plot_blockmodel.py         |    79 +
- .../examples/algorithms/plot_circuits.py           |   103 +
- .../examples/algorithms/plot_davis_club.py         |    43 +
- .../examples/algorithms/plot_dedensification.py    |    92 +
- .../examples/algorithms/plot_girvan_newman.py      |    79 +
- .../algorithms/plot_iterated_dynamical_systems.py  |   210 +
- .../algorithms/plot_krackhardt_centrality.py       |    31 +
- .../algorithms/plot_maximum_independent_set.py     |    44 +
- .../algorithms/plot_parallel_betweenness.py        |    82 +
- .../networkx-3.1/examples/algorithms/plot_rcm.py   |    40 +
- .../networkx-3.1/examples/algorithms/plot_snap.py  |   108 +
- .../examples/algorithms/plot_subgraphs.py          |   170 +
- .../doc/networkx-3.1/examples/basic/README.txt     |     2 +
- .../networkx-3.1/examples/basic/plot_properties.py |    49 +
- .../networkx-3.1/examples/basic/plot_read_write.py |    24 +
- .../examples/basic/plot_simple_graph.py            |    60 +
- .../doc/networkx-3.1/examples/drawing/README.txt   |     2 +
- .../examples/drawing/chess_masters_WCC.pgn.bz2     |   Bin 0 -> 100224 bytes
- .../examples/drawing/knuth_miles.txt.gz            |   Bin 0 -> 20317 bytes
- .../examples/drawing/plot_center_node.py           |    20 +
- .../examples/drawing/plot_chess_masters.py         |   152 +
- .../examples/drawing/plot_custom_node_icons.py     |    75 +
- .../networkx-3.1/examples/drawing/plot_degree.py   |    50 +
- .../networkx-3.1/examples/drawing/plot_directed.py |    46 +
- .../examples/drawing/plot_edge_colormap.py         |    23 +
- .../examples/drawing/plot_ego_graph.py             |    35 +
- .../examples/drawing/plot_eigenvalues.py           |    22 +
- .../examples/drawing/plot_four_grids.py            |    52 +
- .../examples/drawing/plot_house_with_colors.py     |    26 +
- .../examples/drawing/plot_knuth_miles.py           |   142 +
- .../examples/drawing/plot_labels_and_colors.py     |    54 +
- .../examples/drawing/plot_multipartite_graph.py    |    41 +
- .../examples/drawing/plot_node_colormap.py         |    15 +
- .../examples/drawing/plot_rainbow_coloring.py      |    68 +
- .../drawing/plot_random_geometric_graph.py         |    44 +
- .../networkx-3.1/examples/drawing/plot_sampson.py  |    47 +
- .../examples/drawing/plot_selfloops.py             |    29 +
- .../examples/drawing/plot_simple_path.py           |    14 +
- .../examples/drawing/plot_spectral_grid.py         |    58 +
- .../doc/networkx-3.1/examples/drawing/plot_tsp.py  |    52 +
- .../examples/drawing/plot_unix_email.py            |    62 +
- .../examples/drawing/plot_weighted_graph.py        |    44 +
- .../networkx-3.1/examples/drawing/unix_email.mbox  |    84 +
- .../doc/networkx-3.1/examples/graph/README.txt     |     2 +
- .../networkx-3.1/examples/graph/plot_dag_layout.py |    42 +
- .../examples/graph/plot_degree_sequence.py         |    36 +
- .../examples/graph/plot_erdos_renyi.py             |    36 +
- .../graph/plot_expected_degree_sequence.py         |    20 +
- .../networkx-3.1/examples/graph/plot_football.py   |    44 +
- .../examples/graph/plot_karate_club.py             |    25 +
- .../networkx-3.1/examples/graph/plot_morse_trie.py |    97 +
- .../doc/networkx-3.1/examples/graph/plot_mst.py    |    50 +
- .../graph/plot_napoleon_russian_campaign.py        |   133 +
- .../doc/networkx-3.1/examples/graph/plot_roget.py  |    80 +
- .../examples/graph/plot_triad_types.py             |    63 +
- .../doc/networkx-3.1/examples/graph/plot_words.py  |    88 +
- .../networkx-3.1/examples/graph/roget_dat.txt.gz   |   Bin 0 -> 15758 bytes
- .../networkx-3.1/examples/graph/words_dat.txt.gz   |   Bin 0 -> 33695 bytes
- .../doc/networkx-3.1/examples/subclass/README.txt  |     2 +
- .../examples/subclass/plot_antigraph.py            |   192 +
- .../examples/subclass/plot_printgraph.py           |    88 +
- dominique-venv/share/man/man1/isympy.1             |   188 +
- dominique-venv/share/man/man1/ttx.1                |   225 +
- .../CacheControl-0.12.6-py2.py3-none-any.whl       |   Bin 0 -> 28023 bytes
- .../appdirs-1.4.3-py2.py3-none-any.whl             |   Bin 0 -> 18776 bytes
- .../certifi-2019.11.28-py2.py3-none-any.whl        |   Bin 0 -> 164552 bytes
- .../chardet-3.0.4-py2.py3-none-any.whl             |   Bin 0 -> 141487 bytes
- .../colorama-0.4.3-py2.py3-none-any.whl            |   Bin 0 -> 25094 bytes
- .../contextlib2-0.6.0-py2.py3-none-any.whl         |   Bin 0 -> 17188 bytes
- .../distlib-0.3.0-py2.py3-none-any.whl             |   Bin 0 -> 152027 bytes
- .../distro-1.4.0-py2.py3-none-any.whl              |   Bin 0 -> 23898 bytes
- .../html5lib-1.0.1-py2.py3-none-any.whl            |   Bin 0 -> 120020 bytes
- .../python-wheels/idna-2.8-py2.py3-none-any.whl    |   Bin 0 -> 66836 bytes
- .../ipaddr-2.2.0-py2.py3-none-any.whl              |   Bin 0 -> 24287 bytes
- .../lockfile-0.12.2-py2.py3-none-any.whl           |   Bin 0 -> 21972 bytes
- .../msgpack-0.6.2-py2.py3-none-any.whl             |   Bin 0 -> 92927 bytes
- .../packaging-20.3-py2.py3-none-any.whl            |   Bin 0 -> 42242 bytes
- .../pep517-0.8.2-py2.py3-none-any.whl              |   Bin 0 -> 26686 bytes
- .../python-wheels/pip-20.0.2-py2.py3-none-any.whl  |   Bin 0 -> 262440 bytes
- .../pkg_resources-0.0.0-py2.py3-none-any.whl       |   Bin 0 -> 127312 bytes
- .../progress-1.5-py2.py3-none-any.whl              |   Bin 0 -> 17547 bytes
- .../pyparsing-2.4.6-py2.py3-none-any.whl           |   Bin 0 -> 77093 bytes
- .../requests-2.22.0-py2.py3-none-any.whl           |   Bin 0 -> 67543 bytes
- .../retrying-1.3.3-py2.py3-none-any.whl            |   Bin 0 -> 16358 bytes
- .../setuptools-44.0.0-py2.py3-none-any.whl         |   Bin 0 -> 477455 bytes
- .../python-wheels/six-1.14.0-py2.py3-none-any.whl  |   Bin 0 -> 20256 bytes
- .../python-wheels/toml-0.10.0-py2.py3-none-any.whl |   Bin 0 -> 24106 bytes
- .../urllib3-1.25.8-py2.py3-none-any.whl            |   Bin 0 -> 127437 bytes
- .../webencodings-0.5.1-py2.py3-none-any.whl        |   Bin 0 -> 20484 bytes
- .../wheel-0.34.2-py2.py3-none-any.whl              |   Bin 0 -> 35613 bytes
- lzero/entry/train_muzero.py                        |     9 +-
- lzero/mcts/buffer/game_buffer_muzero.py            |    11 +-
- lzero/mcts/tests/test_muzero_game_buffer.py        |     2 +-
- lzero/model/common.py                              |   201 +-
- lzero/model/muzero_model_daev.py                   |   565 +
- lzero/policy/gumbel_muzero.py                      |     2 +-
- lzero/policy/muzero_daev.py                        |   801 +
- lzero/policy/utils.py                              |    20 +-
- .../cartpole/config/cartpole_muzero_config.py      |     2 +-
- 109 files changed, 84896 insertions(+), 13 deletions(-)
-
-commit d37249803c1f9170125c6ef75c15af0ae7f5509f
-Author: Adam Johnson <ajtaken123@gmail.com>
-Date:   Sat Apr 27 23:56:54 2024 -0400
-
-    debugging
-
- .../config/atari_muzero_multigpu_ddp_config.py     |   6 +-
- .../git_diff.txt                                   | 331 ++++++++++++++++
- .../git_log.txt                                    |  48 +++
- .../git_diff.txt                                   | 335 ++++++++++++++++
- .../git_log.txt                                    |  48 +++
- .../git_diff.txt                                   | 336 ++++++++++++++++
- .../git_log.txt                                    |  48 +++
- .../git_diff.txt                                   | 336 ++++++++++++++++
- .../git_log.txt                                    |  48 +++
- .../git_diff.txt                                   | 327 ++++++++++++++++
- .../git_log.txt                                    |  48 +++
- .../git_diff.txt                                   | 336 ++++++++++++++++
- .../git_log.txt                                    |  48 +++
- .../git_diff.txt                                   | 338 ++++++++++++++++
- .../git_log.txt                                    |  48 +++
- .../git_diff.txt                                   | 337 ++++++++++++++++
- .../git_log.txt                                    |  48 +++
- .../git_diff.txt                                   | 337 ++++++++++++++++
- .../git_log.txt                                    |  48 +++
- .../git_diff.txt                                   | 431 +++++++++++++++++++++
- .../git_log.txt                                    |  48 +++
- .../git_diff.txt                                   | 431 +++++++++++++++++++++
- .../git_log.txt                                    |  48 +++
- .../git_diff.txt                                   | 431 +++++++++++++++++++++
- .../git_log.txt                                    |  48 +++
- .../git_diff.txt                                   | 431 +++++++++++++++++++++
- .../git_log.txt                                    |  48 +++
- 27 files changed, 5364 insertions(+), 3 deletions(-)
-
-commit 4640ee3adfad85e346f3409b628d2780cf3716d7
-Author: 蒲源 <2402552459@qq.com>
-Date:   Tue Apr 16 17:53:03 2024 +0800
-
-    polish(pu): polish release.yml
-
- .github/workflows/release.yml | 14 +++++++-------
- 1 file changed, 7 insertions(+), 7 deletions(-)
-
-commit 91ace9be294a4a8d62ab26a5e24f7d49ef36a665
-Author: 蒲源 <2402552459@qq.com>
-Date:   Tue Apr 16 12:11:52 2024 +0800
-
-    polish(pu): polish release.yml
-
- .github/workflows/release.yml | 14 +++++++-------
- 1 file changed, 7 insertions(+), 7 deletions(-)
-
-commit 0264b1baeb2acb91788336dd89d5e393927d51dd
-Author: 蒲源 <2402552459@qq.com>
-Date:   Tue Apr 16 12:02:24 2024 +0800
-
-    polish(pu): polish release.yml
-
- .github/workflows/release.yml | 13 +++++++------
- 1 file changed, 7 insertions(+), 6 deletions(-)
\ No newline at end of file
diff --git a/zoo/atari/entry/atari_eval.py b/zoo/atari/entry/atari_eval.py
index eb61c77..70f317b 100644
--- a/zoo/atari/entry/atari_eval.py
+++ b/zoo/atari/entry/atari_eval.py
@@ -1,5 +1,6 @@
 from lzero.entry import eval_muzero
 import numpy as np
+from lzero.model.muzero_model_daev import MuZeroModel
 
 if __name__ == "__main__":
     """
@@ -21,7 +22,7 @@ if __name__ == "__main__":
 
     # model_path is the path to the trained MuZero model checkpoint.
     # If no path is provided, the script will use the default model.
-    model_path = None
+    model_path = '/home/deep-learning/DAEVZero/zoo/atari/config/daev_data_mz_ctree/Qbert_muzero_ns50_upc1000_rr0.0_ddp_2gpu_seed0/ckpt/ckpt_best.pth.tar'
 
     # seeds is a list of seed values for the random number generator, used to initialize the environment.
     seeds = [0]
@@ -32,6 +33,7 @@ if __name__ == "__main__":
 
     # Setting the type of the environment manager to 'base' for the visualization purposes.
     create_config.env_manager.type = 'base'
+    
     # The number of environments to evaluate concurrently. Set to 1 for visualization purposes.
     main_config.env.evaluator_env_num = 1
     # The total number of evaluation episodes that should be run.
@@ -44,7 +46,7 @@ if __name__ == "__main__":
     # The path where the recorded video will be saved.
     main_config.env.replay_path = './video'
     # The maximum number of steps for each episode during evaluation. This may need to be adjusted based on the specific characteristics of the environment.
-    main_config.env.eval_max_episode_steps = int(20)
+    main_config.env.eval_max_episode_steps = int(1e6)
 
     # These lists will store the mean and total rewards for each seed.
     returns_mean_seeds = []
@@ -57,7 +59,7 @@ if __name__ == "__main__":
             seed=seed,
             num_episodes_each_seed=num_episodes_each_seed,
             print_seed_details=False,
-            model_path=model_path
+            model_path=model_path,
         )
         print(returns_mean, returns)
         returns_mean_seeds.append(returns_mean)
@@ -72,4 +74,4 @@ if __name__ == "__main__":
     print(f"We evaluated a total of {len(seeds)} seeds. For each seed, we evaluated {num_episodes_each_seed} episode(s).")
     print(f"For seeds {seeds}, the mean returns are {returns_mean_seeds}, and the returns are {returns_seeds}.")
     print("Across all seeds, the mean reward is:", returns_mean_seeds.mean())
-    print("=" * 20)
\ No newline at end of file
+    print("=" * 20)